{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT (and other transformer methods) for IR - Data preparation\n",
    "\n",
    "This notebook covers the basic on how to implement a nice pipeline for training and running inference over a IR dataset.\n",
    "We wil use Anserini, with PySerini, to index and retrieve documents over the MsMarco TREC 2019 DL dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dependencies installation\n",
    "First, let's install whÂ§at we need. I highly recommend using something like Conda to manage your environment!\n",
    "\n",
    "We are using Python 3.7 and Cuda 10.1 (If you are using another version, check how to install Pytorch on https://pytorch.org/get-started/locally/#start-locally)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T00:43:24.023779Z",
     "start_time": "2020-04-02T00:43:20.965812Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'anserini'...\n",
      "remote: Enumerating objects: 44, done.\u001b[K\n",
      "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
      "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
      "remote: Total 12581 (delta 15), reused 25 (delta 9), pack-reused 12537\u001b[K\n",
      "Receiving objects: 100% (12581/12581), 18.57 MiB | 15.04 MiB/s, done.\n",
      "Resolving deltas: 100% (7132/7132), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "#Pytorch\n",
    "! conda install -y pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "# ðŸ¤— tokenizer (this gives us A HUGE boost on performance. Tokenizing is the slowest part of the process)\n",
    "! pip install tokenizer\n",
    "# ðŸ¤— Transformer\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T00:43:25.717718Z",
     "start_time": "2020-04-02T00:43:25.599407Z"
    }
   },
   "source": [
    "### Anserini installation.\n",
    "Java is a pain in the ass. That's why you should run these commands on your terminal, not here!\n",
    "\n",
    "```git clone https://github.com/castorini/anserini`\n",
    "curl -s \"https://get.sdkman.io\" | bash\n",
    "source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n",
    "sdk install java\n",
    "sdk use java 11.0.6.hs-adpt # This may change. Check the version of Java 11 that was installed\n",
    "cd anserini\n",
    "mvn clean package -Dmaven.test.skip=true appassembler:assemble\n",
    "```\n",
    "\n",
    "This should be enough to install anserini. If not, check their repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local variables\n",
    "These variables are local to you, and should be eddited accordingly. thinks like path to download the dataset are all set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:51:33.740325Z",
     "start_time": "2020-05-20T20:51:33.733556Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_home = \"/ssd2/arthur/MsMarcoTREC/\"  # Where you want to store the docs\n",
    "anserini_path = \"/ssd2/arthur/bert4IR/anserini\"  # Should be where you downloaded and installed Anserini. Check above!\n",
    "n_threads = 32  # Number of threads to use. Make sure you have more than the number here!\n",
    "\n",
    "\n",
    "def get_path(x):\n",
    "    return os.path.join(data_home, x)\n",
    "\n",
    "\n",
    "if not os.path.isdir(data_home):\n",
    "    os.makedirs(data_home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "We are using the MsMarco TREC 2019 dataset. We should download everything here.\n",
    "\n",
    "If you are running this from the DeepIR machine from WIS, we already have everything there. Ask Arthur where this is and `ln -s` to your path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:51:39.719379Z",
     "start_time": "2020-05-20T20:51:39.698805Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already found file docs/msmarco-docs.trec. Not re-downloading it\n",
      "Already found file docs/msmarco-docs.tsv. Not re-downloading it\n",
      "Already found file queries/msmarco-doctrain-queries.tsv. Not re-downloading it\n",
      "Already found file qrels/msmarco-doctrain-qrels.tsv. Not re-downloading it\n",
      "Already found file queries/msmarco-docdev-queries.tsv. Not re-downloading it\n",
      "Already found file qrels/msmarco-docdev-qrels.tsv. Not re-downloading it\n",
      "Already found file queries/msmarco-test2019-queries.tsv. Not re-downloading it\n",
      "Already found file qrels/2019qrels-docs.txt. Not re-downloading it\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "download_path = \"https://msmarco.blob.core.windows.net/msmarcoranking/\"  # default MsMarco path for downloading data\n",
    "# It sucks to need documents in both .trec and .tsv, but it's easier this way, believe me.\n",
    "files_to_get = [\n",
    "    \"docs/msmarco-docs.trec\",  #docs in trec format\n",
    "    \"docs/msmarco-docs.tsv\",  # docs in tsv format\n",
    "    \"queries/msmarco-doctrain-queries.tsv\",  # train queries\n",
    "    \"qrels/msmarco-doctrain-qrels.tsv\",  # train qrels\n",
    "    \"queries/msmarco-docdev-queries.tsv\",  # dev queries\n",
    "    \"qrels/msmarco-docdev-qrels.tsv\",  # dev qrels\n",
    "    \"queries/msmarco-test2019-queries.tsv\",  # test queries\n",
    "    \"qrels/2019qrels-docs.txt\"  # test qrels\n",
    "]\n",
    "for file in files_to_get:\n",
    "    local_file_path = get_path(file)\n",
    "    if not os.path.isfile(local_file_path):\n",
    "        print(\n",
    "            f\"File {file.split('/')[-1]} not found. Downloading it from the Web\"\n",
    "        )\n",
    "        url_to_fetch_from = download_path + file.split(\"/\")[1] + \".gz\"\n",
    "        # qrels for test comes from NIST, not from Microsoft. Also, no need to uncompress\n",
    "        if file == \"qrels/2019qrels-docs.txt\":\n",
    "            url_to_fetch_from = \"https://trec.nist.gov/data/deep/2019qrels-docs.txt\"\n",
    "            request.urlretrieve(url_to_fetch_from, local_file_path)\n",
    "            continue\n",
    "        # Create dir if it doesn't exist\n",
    "        if not os.path.isdir(\"/\".join(local_file_path.split(\"/\")[:-1])):\n",
    "            os.makedirs(\"/\".join(local_file_path.split(\"/\")[:-1]))\n",
    "        try:\n",
    "            request.urlretrieve(url_to_fetch_from, local_file_path + \".gz\")\n",
    "        except:\n",
    "            print(\n",
    "                f\"Could not fetch {file} from {url_to_fetch_from}. Make sure that's the right URL!\"\n",
    "            )\n",
    "            continue\n",
    "        #Uncompress file. Not needed, but easier. (you could use the gzip lib to open the files...)\n",
    "        with gzip.open(local_file_path + \".gz\", 'rb') as f_in, open(local_file_path, 'wb') as outf:\n",
    "            print(f\"Extracting file {file}\")\n",
    "            shutil.copyfileobj(f_in, outf)\n",
    "            os.remove(local_file_path + \".gz\")\n",
    "    else:\n",
    "        print(f\"Already found file {file}. Not re-downloading it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Anserini Index\n",
    "This may take a while... We are copying the procedure from here: https://github.com/castorini/anserini/blob/master/docs/experiments-msmarco-doc.md.\n",
    "\n",
    "- You will not receive any feedback on the output while the indexing is running. You may chack the progress by running `ls -lah` on the index folder and check if the files are increasing in size.\n",
    "- Alternatively, run the script manually (don't forget to set `JAVA_HOME`) and have some feedback on the terminal. As a sanity check, the index must contain 3,213,835 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:51:52.404762Z",
     "start_time": "2020-05-20T20:51:51.071830Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess, os\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "my_env = os.environ.copy()\n",
    "my_env[\"JAVA_HOME\"] = f\"{home}/.sdkman/candidates/java/11.0.7.hs-adpt\"  #Set right JAVA version\n",
    "\n",
    "command = [\n",
    "    \"sh\", f\"{anserini_path}/target/appassembler/bin/IndexCollection\",  # Invoke Anserini Indexer\n",
    "    \"-collection\", \"TrecCollection\", # Define type of collection (TREC)\n",
    "    \"-generator\", \"LuceneDocumentGenerator\",   # Define type of indice to generate\n",
    "    \"-threads\", str(n_threads),  # Number of threads to use to index\n",
    "    \"-input\", get_path(\"docs/\"),  # File with documents\n",
    "    \"-index\", get_path(\"lucene-index.msmarco-doc.pos+docvectors+rawdocs\"),  # Where to store the index\n",
    "    \"-storePositions\", \"-storeDocvectors\", \"-storeRawDocs\"  # Extra options\n",
    "]\n",
    "\n",
    "# Nothing will output to the shell. You may check progress by running \"ls -lah\" on the idex folder above.\n",
    "# Alternatively, you can run the script manually on a terminal, so you can have some feedback on the indexing process.\n",
    "output = subprocess.run(command,\n",
    "                        stdout=subprocess.PIPE,\n",
    "                        stderr=subprocess.PIPE,\n",
    "                        text=True,\n",
    "                        env=my_env)\n",
    "\n",
    "# Write log to disk.\n",
    "if not os.path.isdir(get_path(\"logs\")):\n",
    "    os.makedirs(get_path(\"logs\"))\n",
    "with open(get_path(\"logs/indexing.log\"), 'w') as f:\n",
    "    f.write(output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T20:52:28.237725Z",
     "start_time": "2020-05-20T20:52:28.232920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(output.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Bert4IR)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
