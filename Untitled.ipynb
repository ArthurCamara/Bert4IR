{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:22:09.363892Z",
     "start_time": "2020-04-16T08:22:09.341556Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\" # Make sure this is the sama JAVA_HOME as the installed version on the previous notebook!\n",
    "data_home = \"/ssd2/arthur/MsMarcoTREC/\"\n",
    "def path(x):\n",
    "    return os.path.join(data_home, x)\n",
    "\n",
    "try:\n",
    "    import pyserini\n",
    "except:\n",
    "    !pip install pyserini==0.8.1.0 # install pyserini\n",
    "try:\n",
    "    import tqdm\n",
    "except:\n",
    "    !pip install tqdm # Good for progress bars!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:22:11.756851Z",
     "start_time": "2020-04-16T08:22:10.553982Z"
    }
   },
   "outputs": [],
   "source": [
    "import jnius_config\n",
    "jnius_config.add_options('-Xmx32G')\n",
    "from pyserini.search import pysearch\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:22:14.128668Z",
     "start_time": "2020-04-16T08:22:13.244066Z"
    }
   },
   "outputs": [],
   "source": [
    "index_path = path(\"lucene-index.msmarco-doc.pos+docvectors+rawdocs\")\n",
    "searcher = pysearch.SimpleSearcher(index_path)\n",
    "relevant_docs = defaultdict(lambda:[])\n",
    "for file in [path(\"qrels/msmarco-doctrain-qrels.tsv\"), path(\"qrels/msmarco-docdev-qrels.tsv\")]:\n",
    "    for line in open(file):\n",
    "        query_id, _, doc_id, rel = line.split()\n",
    "        assert rel == \"1\"\n",
    "        relevant_docs[query_id].append(doc_id)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:28:02.283661Z",
     "start_time": "2020-04-16T08:22:32.063501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already found file /ssd2/arthur/MsMarcoTREC/queries/msmarco-doctrain-queries.tsv. Cowardly refusing to run this again. Will only load querytexts.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdc44c0c0564a2ebe02581a4629334d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Retrieval batches', max=37.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query standingdefinition has less than 2 retrieved docs.\n",
      "query a pantsman has less than 2 retrieved docs.\n",
      "\n",
      "Already found file /ssd2/arthur/MsMarcoTREC/queries/msmarco-docdev-queries.tsv. Cowardly refusing to run this again. Will only load querytexts.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b88b1ad0f44956933c737ab711b38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Retrieval batches', max=1.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('([^\\s\\w]|_)+')\n",
    "printable = set(string.printable)\n",
    "\n",
    "anserini_top_10 = defaultdict(lambda:[])\n",
    "searcher.set_bm25_similarity(0.9, 0.4)\n",
    "pairs_per_split = defaultdict(lambda: [])\n",
    "threads = 50 # Number of Threads to use when retrieving\n",
    "k = 10       # Number of documents to retrieve \n",
    "neg_samples = 2 # Number of negatives samples to use\n",
    "batch_size = 10000 # Batch size for each retrieval step on Anserini\n",
    "\n",
    "query_texts = dict()\n",
    "for split in [\"train\", \"dev\"]:\n",
    "    file_path = path(f\"queries/msmarco-doc{split}-queries.tsv\")\n",
    "    run_search=True\n",
    "    if os.path.isfile(file_path):\n",
    "        print(f\"Already found file {file_path}. Cowardly refusing to run this again. Will only load querytexts.\")\n",
    "        pairs_per_split[split] = pickle.load(open(path(f\"{split}_triples.pkl\"), 'rb'))\n",
    "        run_search = False\n",
    "    number_of_queries = int(subprocess.run(f\"wc -l {file_path}\".split(), capture_output=True).stdout.split()[0])\n",
    "    number_of_batches = math.ceil(number_of_queries/batch_size)\n",
    "    pbar = tqdm(total=number_of_batches, desc=\"Retrieval batches\")\n",
    "    queries = []\n",
    "    query_ids = []\n",
    "    for idx, line in enumerate(open(file_path, encoding=\"utf-8\")):\n",
    "        query_id, query = line.strip().split(\"\\t\")\n",
    "        query_ids.append(query_id)\n",
    "        query = unicodedata.normalize(\"NFKD\", query) # Force queries into UTF-8\n",
    "        query = pattern.sub(' ',query) # Remove non-ascii characters. It clears up most of the issues we may find on the query datasets\n",
    "        queries.append(query)\n",
    "        if len(queries) == batch_size or idx == number_of_queries-1:\n",
    "            results = searcher.batch_search(queries, query_ids, k=k, threads=threads)\n",
    "            pbar.update()\n",
    "            for query, query_id in zip(queries, query_ids):\n",
    "                retrieved_docs_ids = [hit.docid for hit in results[query_id]]\n",
    "                relevant_docs_for_query = relevant_docs[query_id]\n",
    "                retrieved_non_relevant_documents = set(retrieved_docs_ids).difference(set(relevant_docs_for_query))\n",
    "                  \n",
    "                if len(retrieved_non_relevant_documents) < 2:\n",
    "                    print(f\"query {query} has less than 2 retrieved docs.\")\n",
    "                    continue\n",
    "                random_negative_samples = random.sample(retrieved_non_relevant_documents, neg_samples)\n",
    "                pairs_per_split[split] += [(query_id, doc_id, 1) for doc_id in relevant_docs_for_query]\n",
    "                pairs_per_split[split] += [(query_id, doc_id, 0) for doc_id in random_negative_samples]\n",
    "            queries = []\n",
    "            query_ids = []\n",
    "    pickle.dump(pairs_per_split[split], open(path(f\"{split}_triples.pkl\"), 'wb'))\n",
    "    pbar.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Bert4IR)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
